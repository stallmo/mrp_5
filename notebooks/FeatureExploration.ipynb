{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loading_routines import *\n",
    "from preprocessing import *\n",
    "from feature_engineering import *\n",
    "from mov_ampl import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('../data/behavior_AND_personality_dataset/joints/*.xml')\n",
    "#print all_files\n",
    "all_files_new = glob.glob('../data/data_recordings_master/joints/*.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 tracks for \"subject6\"\n",
      "Loaded 1000 tracks for \"subject6\"\n",
      "Loaded 2000 tracks for \"subject6\"\n",
      "Loaded 3000 tracks for \"subject6\"\n",
      "Loaded 4000 tracks for \"subject6\"\n",
      "Loaded 5000 tracks for \"subject6\"\n",
      "Loaded 6000 tracks for \"subject6\"\n",
      "Loaded 7000 tracks for \"subject6\"\n",
      "Loaded 8000 tracks for \"subject6\"\n",
      "Loaded 9000 tracks for \"subject6\"\n",
      "Loaded 10000 tracks for \"subject6\"\n",
      "Loaded 11000 tracks for \"subject6\"\n",
      "Loaded 12000 tracks for \"subject6\"\n",
      "Loaded 13000 tracks for \"subject6\"\n",
      "Loaded 14000 tracks for \"subject6\"\n",
      "Loaded 15000 tracks for \"subject6\"\n",
      "Loaded 16000 tracks for \"subject6\"\n",
      "Loaded 17000 tracks for \"subject6\"\n",
      "Loaded 18000 tracks for \"subject6\"\n",
      "Loaded 19000 tracks for \"subject6\"\n",
      "Loaded 20000 tracks for \"subject6\"\n",
      "Loaded 21000 tracks for \"subject6\"\n",
      "Loaded 22000 tracks for \"subject6\"\n",
      "Loaded 23000 tracks for \"subject6\"\n",
      "Loaded 24000 tracks for \"subject6\"\n",
      "Loaded 25000 tracks for \"subject6\"\n",
      "Loaded 26000 tracks for \"subject6\"\n",
      "Loaded 27000 tracks for \"subject6\"\n",
      "Loaded 0 tracks for \"subject11\"\n",
      "Loaded 1000 tracks for \"subject11\"\n",
      "Loaded 2000 tracks for \"subject11\"\n",
      "Loaded 3000 tracks for \"subject11\"\n",
      "Loaded 4000 tracks for \"subject11\"\n",
      "Loaded 5000 tracks for \"subject11\"\n",
      "Loaded 6000 tracks for \"subject11\"\n",
      "Loaded 7000 tracks for \"subject11\"\n",
      "Loaded 8000 tracks for \"subject11\"\n",
      "Loaded 9000 tracks for \"subject11\"\n",
      "Loaded 10000 tracks for \"subject11\"\n",
      "Loaded 11000 tracks for \"subject11\"\n",
      "Loaded 12000 tracks for \"subject11\"\n",
      "Loaded 13000 tracks for \"subject11\"\n",
      "Loaded 14000 tracks for \"subject11\"\n",
      "Loaded 15000 tracks for \"subject11\"\n",
      "Loaded 16000 tracks for \"subject11\"\n",
      "Loaded 17000 tracks for \"subject11\"\n",
      "Loaded 18000 tracks for \"subject11\"\n",
      "Loaded 19000 tracks for \"subject11\"\n",
      "Loaded 20000 tracks for \"subject11\"\n",
      "Loaded 21000 tracks for \"subject11\"\n",
      "Loaded 22000 tracks for \"subject11\"\n",
      "Loaded 23000 tracks for \"subject11\"\n",
      "Loaded 24000 tracks for \"subject11\"\n",
      "Loaded 25000 tracks for \"subject11\"\n",
      "Loaded 26000 tracks for \"subject11\"\n",
      "Loaded 27000 tracks for \"subject11\"\n",
      "Loaded 28000 tracks for \"subject11\"\n",
      "Loaded 29000 tracks for \"subject11\"\n",
      "Loaded 0 tracks for \"subject9\"\n",
      "Loaded 1000 tracks for \"subject9\"\n",
      "Loaded 2000 tracks for \"subject9\"\n",
      "Loaded 3000 tracks for \"subject9\"\n",
      "Loaded 4000 tracks for \"subject9\"\n",
      "Loaded 5000 tracks for \"subject9\"\n",
      "Loaded 6000 tracks for \"subject9\"\n",
      "Loaded 7000 tracks for \"subject9\"\n",
      "Loaded 8000 tracks for \"subject9\"\n",
      "Loaded 9000 tracks for \"subject9\"\n",
      "Loaded 10000 tracks for \"subject9\"\n",
      "Loaded 11000 tracks for \"subject9\"\n",
      "Loaded 12000 tracks for \"subject9\"\n",
      "Loaded 13000 tracks for \"subject9\"\n",
      "Loaded 14000 tracks for \"subject9\"\n",
      "Loaded 15000 tracks for \"subject9\"\n",
      "Loaded 16000 tracks for \"subject9\"\n",
      "Loaded 17000 tracks for \"subject9\"\n",
      "Loaded 18000 tracks for \"subject9\"\n",
      "Loaded 19000 tracks for \"subject9\"\n",
      "Loaded 20000 tracks for \"subject9\"\n",
      "Loaded 21000 tracks for \"subject9\"\n",
      "Loaded 22000 tracks for \"subject9\"\n",
      "Loaded 23000 tracks for \"subject9\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-068e4119ca90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_subjects_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_df_from_xml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_files\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mall_files_new\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/morris/MaastrichtUniversity/2017_period1/ResearchProject/git/mrp_5/code/loading_routines.pyc\u001b[0m in \u001b[0;36mload_df_from_xml\u001b[0;34m(path_to_xml, n_joints)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mjoint\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_joint_no\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_joints\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfirst_joint_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcoord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;31m#print(len(row))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdf_ind\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_subjects_dfs = [load_df_from_xml(f) for f in all_files+all_files_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "#### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_prep = [remove_outliers(df, low_percentil=0.05, high_percentil=0.95) for df in all_subjects_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep_norm = [normalize_data(df) for df in dfs_prep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_prep_norm[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect frames with little movement\n",
    "\n",
    "* either of the head coordinates in a sequence moves more than 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#is a list of lists\n",
    "# \"little movement\" not little enough?\n",
    "dfs_little_movement = [get_sequences_with_little_movement(df, variables_to_check=['head_x', 'head_y', 'head_z'], max_mov=0.03) for df in df_prep_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df in dfs_little_movement:\n",
    "    print 'Length of sequences with little movement for ', df[0]['subject'].iloc[0]\n",
    "    print '*'*5\n",
    "    for d in df:\n",
    "        print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfs_little_movement[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_little_movement[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_little_movement_per_person = [pd.concat(df) for df in dfs_little_movement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df in all_little_movement_per_person:\n",
    "    print len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Posture Features for upper joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_joints = ['head_x', 'head_y', 'head_z',\n",
    "              'neck_x', 'neck_y', 'neck_z',\n",
    "              'spineShoulder_x', 'spineShoulder_y', 'spineShoulder_z',\n",
    "              'shoulderR_x', 'shoulderR_y', 'shoulderR_z',\n",
    "              'elbowR_x', 'elbowR_y', 'elbowR_z',\n",
    "              'wristR_x', 'wristR_y', 'wristR_z',\n",
    "              'handR_x', 'handR_y', 'handR_z',\n",
    "              'shoulderL_x', 'shoulderL_y', 'shoulderL_z',\n",
    "              'elbowL_x', 'elbowL_y', 'elbowL_z',\n",
    "              'wristL_x', 'wristL_y', 'wristL_z',\n",
    "              'handL_x', 'handL_y', 'handL_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posture_per_person = [calculate_joint_differences(df, only_for_columns=upper_joints) for df in all_little_movement_per_person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Total posture features: ', len(posture_per_person[0].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posture_per_person[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_posture_features = []\n",
    "for i, df in enumerate(posture_per_person):\n",
    "    pos_mean = pd.DataFrame([df.mean().values], columns=df.mean().index)\n",
    "    subject = all_little_movement_per_person[i].iloc[0]['subject']\n",
    "    pos_mean['subject'] = subject\n",
    "    mean_posture_features.append(pos_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_posture_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posture_feat_df = pd.concat(mean_posture_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at specific set of posture features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_posture = ['head_'+coord+'-shoulderL_'+coord for coord in ['x', 'y', 'z']]+['shoulderR_'+coord+'-head_'+coord for coord in ['x', 'y', 'z']]+['shoulderR_'+coord+'-shoulderL_'+coord for coord in ['x', 'y', 'z']]+['wristR_'+coord+'-neck_'+coord for coord in ['x', 'y', 'z']]\n",
    "specific_posture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get subject number back (was lost while calculating posture features)\n",
    "for i, df in enumerate(posture_per_person):\n",
    "    subject = all_little_movement_per_person[i].iloc[0]['subject']\n",
    "    df['subject'] = [subject]*len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df in posture_per_person:\n",
    "    print'Means for subject ', df['subject'].iloc[0]\n",
    "    print '*'*5\n",
    "    print df[specific_posture].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "len(posture_per_person[0][specific_posture])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_posture_df = pd.concat(posture_per_person)\n",
    "len(big_posture_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fig=plt.figure(figsize=(20, 20))\n",
    "big_posture_df.boxplot(column = specific_posture,\n",
    "                       by='subject', figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extract Movement Amplitude features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_feature_dfs = []\n",
    "still_joints = ['spineMid', 'neck']\n",
    "mov_joints = ['handR', 'handL', 'wristL', 'head']\n",
    "for df in all_subjects_dfs:\n",
    "    subject = df['subject'].iloc[0]\n",
    "    subject_amp_dfs = []\n",
    "    for still in still_joints:\n",
    "        for mov in mov_joints:            \n",
    "            subject_amp_dfs.append(mov_amplitude(df, s_joint=still, m_joint=mov).reset_index())\n",
    "    amp_df = pd.concat(subject_amp_dfs, axis=1)\n",
    "    amp_df['subject'] = subject\n",
    "    #print(amp_df)\n",
    "    amplitude_feature_dfs.append(amp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_amplitude(df, s_joint=still, m_joint=mov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_df = pd.concat(amplitude_feature_dfs).reset_index()\n",
    "amplitude_df.drop('index', axis=1, inplace=True)\n",
    "amplitude_df.drop('level_0', axis=1, inplace=True)\n",
    "amplitude_df.head()\n",
    "#amplitude_df.join(mean_posture_features, on='subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = amplitude_df.merge(posture_feat_df, on='subject')\n",
    "all_features.loc[17, 'subject'] = 'subject17' #fix naming error\n",
    "#all_features['subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var_features = list(all_features.var().index[(all_features.var()>0.001).values])\n",
    "var_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features[var_features].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(16, 16))\n",
    "sns.heatmap(np.abs(all_features[var_features].corr()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_files1 = glob.glob('../data/behavior_AND_personality_dataset/binary/*.txt')\n",
    "sensor_files2 = glob.glob('../data/data_recordings_master/binary/*.txt')\n",
    "sensor_files1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: extract sensor features and concat to feature dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "import speed_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: extract speed features and concat to feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load personality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import personality_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personality_df = personality_data.personality_data_to_data_frame('../data/behavior_AND_personality_dataset/big5_personality_result.txt')\n",
    "names = [s.replace('_', '') for s in personality_df['name']]\n",
    "personality_df = personality_df.transpose()\n",
    "#print(personality_df)\n",
    "cols = personality_df.index[11:]\n",
    "#print(cols)\n",
    "personality_df = normalize_data(personality_df[11:], columns=personality_df.columns).transpose()\n",
    "personality_df.columns = cols\n",
    "personality_df['subject'] = names\n",
    "personality_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data_recordings_master/personality.txt', 'r') as f:\n",
    "    rows = [row.replace('\\r', '').replace('\\n', '').split() for row in f.readlines()]\n",
    "#print(rows)\n",
    "new_rows = []\n",
    "for row in rows:\n",
    "    new_row = []\n",
    "    for v in row:\n",
    "        if v.startswith('s'):\n",
    "            new_row.append(v)\n",
    "        else:\n",
    "            new_row.append(int(v))\n",
    "    new_rows.append(new_row)\n",
    "            \n",
    "columns = personality_data.get_column_names()[:11]\n",
    "personality_df2 = pd.DataFrame(new_rows, columns=columns)\n",
    "personality_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraversion = 5 - personality_df2['question_1'].values + personality_df2['question_6'].values\n",
    "agreeableness = personality_df2['question_2'].values+ (5-personality_df2['question_7'].values)\n",
    "conscientiousness = 5-personality_df2['question_3'].values + personality_df2['question_8'].values\n",
    "neuroticism = 5-personality_df2['question_4'].values + personality_df2['question_9'].values\n",
    "openess = 5-personality_df2['question_5'].values+personality_df2['question_10'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personality_df2['extraversion'] = extraversion\n",
    "personality_df2['agreeableness'] = agreeableness\n",
    "personality_df2['conscientiousness'] = conscientiousness\n",
    "personality_df2['neuroticism'] = neuroticism\n",
    "personality_df2['openness_to_experience'] = openess\n",
    "personality_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = [s.replace('_', '') for s in personality_df2['name']]\n",
    "personality_df2 = personality_df2.transpose()\n",
    "#print(personality_df)\n",
    "cols = personality_df2.index[11:]\n",
    "#print(cols)\n",
    "personality_df2 = normalize_data(personality_df2[11:], columns=personality_df2.columns).transpose()\n",
    "personality_df2.columns = cols\n",
    "personality_df2['subject'] = names\n",
    "personality_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "personality_full = pd.concat([personality_df, personality_df2], axis=0)\n",
    "personality_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_pers_df = all_features[var_features+['subject']].merge(personality_full, on = 'subject')\n",
    "big5 = ['extraversion', 'agreeableness', 'conscientiousness', 'neuroticism', 'openness_to_experience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_pers_df.corr()[big5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(np.abs(feat_pers_df.corr()[big5]), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
